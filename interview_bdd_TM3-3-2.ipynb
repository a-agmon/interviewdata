{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Interview-Answers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-agmon/interviewdata/blob/main/interview_bdd_TM3-3-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Please start by running the following cells that will download the data and the Spark environment.\n",
        "#### Questions start after this part"
      ],
      "metadata": {
        "id": "zt00VArS0WAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkKbxoXeythI"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-01\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-02\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-03"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf transactions-postproc\n",
        "!rm -rf daily-transactions\n",
        "!mkdir daily-transactions\n",
        "!mv daily-*2020* daily-transactions/"
      ],
      "metadata": {
        "id": "srxA-UPRy4r8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A94t8YSBzNyy",
        "outputId": "6e59dabe-12b8-4bf4-86fe-7ae16c4ac412"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.8.0-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvB2Zqkzz-6L",
        "outputId": "3d466719-c5da-4c8f-fb0e-f6d7c846d5c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "ivG5oW6o0XmB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting a spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "conf = SparkConf()"
      ],
      "metadata": {
        "id": "thKuQtUgAfbp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please also run these - they build the data structure for the first question"
      ],
      "metadata": {
        "id": "OMOGSnxpPewG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packs =  [\n",
        "    {'pack':1, 'pack_start_date':123456, 'pack_end_date':123460, 'pack_installs':10},\n",
        "    {'pack':2, 'pack_start_date':123460, 'pack_end_date':123470, 'pack_installs':5},\n",
        "    {'pack':3, 'pack_start_date':123470, 'pack_end_date':123475, 'pack_installs':10}]\n",
        "\n",
        "consumption = [\n",
        "    {'account':'AB','install_date':123459, 'installs':10},\n",
        "    {'account':'AB','install_date':123465, 'installs':5},\n",
        "    {'account':'AB','install_date':123466, 'installs':3}]"
      ],
      "metadata": {
        "id": "0zW_MCGfPYAE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### * START HERE *"
      ],
      "metadata": {
        "id": "QPBj0VDsiU27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run these and view the generated tables\n",
        "\n",
        "packs_df = spark.createDataFrame(packs)\n",
        "consumption_df = spark.createDataFrame(consumption)\n",
        "\n",
        "print(\"Packages table\\n\")\n",
        "packs_df.show()\n",
        "print(\"consumption table\\n\")\n",
        "consumption_df.show()\n",
        "\n",
        "packs_df.createOrReplaceTempView(\"packs\")\n",
        "consumption_df.createOrReplaceTempView(\"consumption\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3fUaCg6Pa53",
        "outputId": "ea00d7b9-67fc-4712-9a74-37c1bc983c6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages table\n",
            "\n",
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n",
            "consumption table\n",
            "\n",
            "+-------+------------+--------+\n",
            "|account|install_date|installs|\n",
            "+-------+------------+--------+\n",
            "|     AB|      123459|      10|\n",
            "|     AB|      123465|       5|\n",
            "|     AB|      123466|       3|\n",
            "+-------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions for Q1\n",
        "\n",
        "The **Packages** table represents packages that customers purchase.\n",
        "Each package has an ID, a start and end date (represented by a number), and a number of installs that the package includes.\n",
        "\n",
        "The **Consumption** table shows us how many installs each account used and when. When we get consumption data for a user, then we need to check according to the date, which package the user used. A user can only have one package in any given time. \n",
        "\n",
        "The report we need to calculate needs to show how much installs a user used from each of its packages, and how many installs remain in each package the user purchased "
      ],
      "metadata": {
        "id": "jy8-_ThxpBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an example to how a spark query can run\n",
        "spark.sql(\"select * from packs\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tilfLIHGPp4L",
        "outputId": "d3eeab43-5dad-409a-bdee-a13bc7490f28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example for the report we want to see\n",
        "\n",
        "\n",
        "```\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|account|pack|InstallsInPackage|InstallsUsed|InstallsDelta|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|     AB|   1|               10|          10|            0|\n",
        "|     AB|   2|                5|           8|           -3|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "JzdbqT5NQAVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqlQuery = \"\"\"\n",
        "\n",
        "SELECT 1 + 1\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RxAxcYiSP8Ag"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(sqlQuery).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLHy04x1QcRb",
        "outputId": "be92ce86-769d-4c11-e31c-0717fd18dfcf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|(1 + 1)|\n",
            "+-------+\n",
            "|      2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instruction for Q2**\n",
        "\n",
        "A developer on the team wrote an ETL that runs once a day as a Spark job.\n",
        "Every day it reads a csv file that shows the total value of each customer's transactions of that day and write them as a parquet file partitioned by date and customer id.\n",
        "Below you can see an example of the CSV file. Note that each customer has one entry that represents the total sum of transaction value it did on that day.\n",
        "\n",
        "However, sometimes the csv file contains a correction for a sum reported in the past. \n",
        "\n",
        "for example - This file represents the transactions on the 1/10. You can see that **customer 1002** has 2 entries. One for the 1/10 and one for 30/9. This means that the total sum of transactions the customer did on the 1/10 is 70, but also that the total sum of transaction it did on the 30/9 was 40 and that this sum should **replace** the value already reported on the 30/9. \n",
        "\n",
        "\n",
        "```\n",
        "current date file: 2020-10-01\n",
        "\n",
        "date,customer,price\n",
        "2020-10-01,1000,40\n",
        "2020-10-01,1001,10\n",
        "2020-09-30,1002,40\n",
        "2020-10-01,1002,70\n",
        "2020-10-01,1003,10\n",
        "2020-09-29,1004,10\n",
        "2020-10-01,1004,10\n",
        "```\n",
        "\n",
        "After the transformations files written in this partitioning scheme based on date and customer id\n",
        "\n",
        "```\n",
        "|_date=2000-1-1\n",
        "|___customer=100\n",
        "|_______file.p\n",
        "|___customer=101\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZIXrlZ4d0_c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the folder that the prq files are written to\n",
        "# before running the ETL this should be cleared \n",
        "!rm -rf transactions-postproc/"
      ],
      "metadata": {
        "id": "x0O9zdz92Fb_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function represents the ETL. It runs once a day with a string represening the current day. \n",
        "\n",
        "It reads the csv file, does some transformations, and write it."
      ],
      "metadata": {
        "id": "H5W2GkNB86BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_etl(current_date): \n",
        "\n",
        "  df = spark.read.option(\"header\",True).csv(f\"daily-transactions/daily-transactions-{current_date}\")\n",
        "  \n",
        "  df = df.withColumn(\"priceNumeric\", F.col(\"price\").astype(IntegerType()))\n",
        "  \n",
        "  # some other transformation code \n",
        "\n",
        "  df.write \\\n",
        "  .option(\"header\",True) \\\n",
        "  .partitionBy(\"date\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .parquet(\"transactions-postproc\")"
      ],
      "metadata": {
        "id": "ZQ2RGvAp04Da"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell simulate the ETL running over 3 days for testing purposes"
      ],
      "metadata": {
        "id": "-qLM9lIH9QYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# takes a minute to run!\n",
        "days = ['2020-10-01', '2020-10-02', '2020-10-03']\n",
        "\n",
        "for date_str in days:\n",
        "  run_etl(date_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKu7BOc61aQK",
        "outputId": "bc612d55-e8ff-499e-b9b6-96a48caeb514"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 62.7 ms, sys: 14.4 ms, total: 77.1 ms\n",
            "Wall time: 6.25 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the two lines below to test the results that should sum how much did the company made each day from all the customers"
      ],
      "metadata": {
        "id": "a2oKbMy69qsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")"
      ],
      "metadata": {
        "id": "wI9C2VRR3Jcd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"date\") \\\n",
        ".sum(\"priceNumeric\") \\\n",
        ".sort(\"date\") \\\n",
        ".show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNeTRt8j3RSl",
        "outputId": "97aa24ce-f476-4133-8dbe-bf746bf92200"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|date      |sum(priceNumeric)|\n",
            "+----------+-----------------+\n",
            "|2020-10-01|5120             |\n",
            "|2020-10-02|5190             |\n",
            "|2020-10-03|36610            |\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finance saw these results, and told us that there is an error here. They did the calculations manually and told us that it is supposed to be like this:\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "+----------+-----------------+\n",
        "|date      |sum(priceNumeric)|\n",
        "+----------+-----------------+\n",
        "|2020-09-29|4880             |\n",
        "|2020-09-30|9790             |\n",
        "|2020-10-01|35330            |\n",
        "|2020-10-02|32940            |\n",
        "|2020-10-03|36610            |\n",
        "+----------+-----------------+\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Please help us find the bug in the code above, and return the right results"
      ],
      "metadata": {
        "id": "f3Lz4jGe94Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions for Q3\n",
        "\n",
        "A developer on the team was running the follwing line in a function for logging purposes, and the job crashed with out of memory exception. \n",
        "The developer says that the cluster has many workers with a lot of memory and disk and still the job crashes.\n",
        "Can you help explain how come this line makes the job crash with OOM even though the cluster is huge?"
      ],
      "metadata": {
        "id": "647kSHW1B3AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def someFunc():\n",
        "  #.....\n",
        "  for row in df.collect():\n",
        "    print(f'Customerr{row[\"customer\"]} => Paid {row[\"price\"]}')"
      ],
      "metadata": {
        "id": "ViPXYkTOBLp_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instruction for Q4\n"
      ],
      "metadata": {
        "id": "9ZUap62QwEEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our developer had to join the results with dimentional table of categories. The join works, but its a bit slow, see if you can understand why and whether it can run faster"
      ],
      "metadata": {
        "id": "Eevy7a3iwK0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the first table\n",
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")\n",
        "# create the next table\n",
        "ratesCategory = [('Small Money',10),('Some Money',20),('Nice Value',40),('BigMoney',70)]\n",
        "categoriesDF = spark.createDataFrame(ratesCategory,['Category','Value'])\n",
        "categoriesDF.show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEOOrFdSwCle",
        "outputId": "4747c59d-a9fe-441b-e555-cb4aac8c19ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|Category   |Value|\n",
            "+-----------+-----+\n",
            "|Small Money|10   |\n",
            "|Some Money |20   |\n",
            "|Nice Value |40   |\n",
            "|BigMoney   |70   |\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "bigDF = categoriesDF.join(broadcast(df), categoriesDF.Value == df.priceNumeric)\n",
        "bigDF.show(5, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt2Q1pj1w6F5",
        "outputId": "d9490051-3c16-4ffa-d262-7cdb3a69d3ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+--------+-----+------------+----------+\n",
            "|Category   |Value|customer|price|priceNumeric|date      |\n",
            "+-----------+-----+--------+-----+------------+----------+\n",
            "|Small Money|10   |1981    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1978    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1967    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1949    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1937    |10   |10          |2020-10-01|\n",
            "+-----------+-----+--------+-----+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PG1qCc_bx8wL"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}