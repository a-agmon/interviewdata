{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-agmon/interviewdata/blob/main/interview_bdd_TM3-3-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt00VArS0WAx"
      },
      "source": [
        "\n",
        "### Please start by running the following cells that will download the data and the Spark environment.\n",
        "#### Questions start after this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkKbxoXeythI"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-01\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-02\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "srxA-UPRy4r8"
      },
      "outputs": [],
      "source": [
        "!rm -rf transactions-postproc\n",
        "!rm -rf daily-transactions\n",
        "!mkdir daily-transactions\n",
        "!mv daily-*2020* daily-transactions/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A94t8YSBzNyy",
        "outputId": "6e59dabe-12b8-4bf4-86fe-7ae16c4ac412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tree is already the newest version (1.8.0-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tree\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.3-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvB2Zqkzz-6L",
        "outputId": "3d466719-c5da-4c8f-fb0e-f6d7c846d5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ivG5oW6o0XmB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "thKuQtUgAfbp"
      },
      "outputs": [],
      "source": [
        "# starting a spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "conf = SparkConf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMOGSnxpPewG"
      },
      "source": [
        "Please also run these - they build the data structure for the first question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0zW_MCGfPYAE"
      },
      "outputs": [],
      "source": [
        "packs =  [\n",
        "    {'pack':1, 'pack_start_date':123456, 'pack_end_date':123460, 'pack_installs':10},\n",
        "    {'pack':2, 'pack_start_date':123460, 'pack_end_date':123470, 'pack_installs':5},\n",
        "    {'pack':3, 'pack_start_date':123470, 'pack_end_date':123475, 'pack_installs':10}]\n",
        "\n",
        "consumption = [\n",
        "    {'account':'AB','install_date':123459, 'installs':10},\n",
        "    {'account':'AB','install_date':123465, 'installs':5},\n",
        "    {'account':'AB','install_date':123466, 'installs':3}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBj0VDsiU27"
      },
      "source": [
        "### * START HERE *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3fUaCg6Pa53",
        "outputId": "ea00d7b9-67fc-4712-9a74-37c1bc983c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Packages table\n",
            "\n",
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n",
            "consumption table\n",
            "\n",
            "+-------+------------+--------+\n",
            "|account|install_date|installs|\n",
            "+-------+------------+--------+\n",
            "|     AB|      123459|      10|\n",
            "|     AB|      123465|       5|\n",
            "|     AB|      123466|       3|\n",
            "+-------+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# run these and view the generated tables\n",
        "\n",
        "packs_df = spark.createDataFrame(packs)\n",
        "consumption_df = spark.createDataFrame(consumption)\n",
        "\n",
        "print(\"Packages table\\n\")\n",
        "packs_df.show()\n",
        "print(\"consumption table\\n\")\n",
        "consumption_df.show()\n",
        "\n",
        "packs_df.createOrReplaceTempView(\"packs\")\n",
        "consumption_df.createOrReplaceTempView(\"consumption\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy8-_ThxpBY2"
      },
      "source": [
        "## Instructions for Q1\n",
        "\n",
        "The **Packages** table represents packages that customers purchase.\n",
        "Each package has an ID, a start and end date (represented by a number), and a number of installs that the package includes.\n",
        "\n",
        "The **Consumption** table shows us how many installs each account used and when. When we get consumption data for a user, then we need to check according to the date, which package the user used. A user can only have one package in any given time. \n",
        "\n",
        "The report we need to calculate needs to show how much installs a user used from each of its packages, and how many installs remain in each package the user purchased "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tilfLIHGPp4L",
        "outputId": "d3eeab43-5dad-409a-bdee-a13bc7490f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# an example to how a spark query can run\n",
        "spark.sql(\"select * from packs\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzdbqT5NQAVc"
      },
      "source": [
        "An example for the report we want to see (the data here is diff than the above so results will not be the same)\n",
        "\n",
        "\n",
        "```\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|account|pack|InstallsInPackage|InstallsUsed|InstallsDelta|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|     XY|   7|               12|          10|            2|\n",
        "|     XZ|   9|                9|           8|           -1|\n",
        "|     XZ|   8|                3|           6|           -3|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RxAxcYiSP8Ag"
      },
      "outputs": [],
      "source": [
        "# you can also use spark api directly if you want\n",
        "sqlQuery = \"\"\"\n",
        "\n",
        "SELECT 1 + 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLHy04x1QcRb",
        "outputId": "be92ce86-769d-4c11-e31c-0717fd18dfcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|(1 + 1)|\n",
            "+-------+\n",
            "|      2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(sqlQuery).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXrlZ4d0_c_"
      },
      "source": [
        "### **Instruction for Q2**\n",
        "\n",
        "A developer on the team wrote an ETL that runs once a day as a Spark job.\n",
        "Every day it reads a csv file that shows the total value of each customer's transactions of that day and write them as a parquet file partitioned by date and customer id.\n",
        "Below you can see an example of the CSV file. Note that each customer has one entry that represents the total sum of transaction value it did on that day.\n",
        "\n",
        "However, sometimes the csv file contains a correction for a sum reported in the past. \n",
        "\n",
        "for example - This file represents the transactions on the 1/10. You can see that **customer 1002** has 2 entries. One for the 1/10 and one for 30/9. This means that the total sum of transactions the customer did on the 1/10 is 70, but also that the total sum of transaction it did on the 30/9 was 40 and that this sum should **replace** the value already reported on the 30/9. \n",
        "\n",
        "\n",
        "```\n",
        "current date file: 2020-10-01\n",
        "\n",
        "date,customer,price\n",
        "2020-10-01,1000,40\n",
        "2020-10-01,1001,10\n",
        "2020-09-30,1002,40\n",
        "2020-10-01,1002,70\n",
        "2020-10-01,1003,10\n",
        "2020-09-29,1004,10\n",
        "2020-10-01,1004,10\n",
        "```\n",
        "\n",
        "After the transformations files written in this partitioning scheme based on date and customer id\n",
        "\n",
        "```\n",
        "|_date=2000-1-1\n",
        "|___customer=100\n",
        "|_______file.p\n",
        "|___customer=101\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x0O9zdz92Fb_"
      },
      "outputs": [],
      "source": [
        "# This is the folder that the prq files are written to\n",
        "# before running the ETL this should be cleared \n",
        "!rm -rf transactions-postproc/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5W2GkNB86BZ"
      },
      "source": [
        "This function represents the ETL. It runs once a day with a string represening the current day. \n",
        "\n",
        "It reads the csv file, does some transformations, and write it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZQ2RGvAp04Da"
      },
      "outputs": [],
      "source": [
        "def run_etl(current_date): \n",
        "\n",
        "  df = spark.read.option(\"header\",True).csv(f\"daily-transactions/daily-transactions-{current_date}\")\n",
        "  \n",
        "  df = df.withColumn(\"priceNumeric\", F.col(\"price\").astype(IntegerType()))\n",
        "  \n",
        "  # some other transformation code \n",
        "\n",
        "  df.write \\\n",
        "  .option(\"header\",True) \\\n",
        "  .partitionBy(\"date\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .parquet(\"transactions-postproc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qLM9lIH9QYp"
      },
      "source": [
        "This cell simulate the ETL running over 3 days for testing purposes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKu7BOc61aQK",
        "outputId": "bc612d55-e8ff-499e-b9b6-96a48caeb514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 62.7 ms, sys: 14.4 ms, total: 77.1 ms\n",
            "Wall time: 6.25 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# takes a minute to run!\n",
        "days = ['2020-10-01', '2020-10-02', '2020-10-03']\n",
        "\n",
        "for date_str in days:\n",
        "  run_etl(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2oKbMy69qsQ"
      },
      "source": [
        "Run the two lines below to test the results that should sum how much did the company made each day from all the customers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wI9C2VRR3Jcd"
      },
      "outputs": [],
      "source": [
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNeTRt8j3RSl",
        "outputId": "97aa24ce-f476-4133-8dbe-bf746bf92200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------------+\n",
            "|date      |sum(priceNumeric)|\n",
            "+----------+-----------------+\n",
            "|2020-10-01|5120             |\n",
            "|2020-10-02|5190             |\n",
            "|2020-10-03|36610            |\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"date\") \\\n",
        ".sum(\"priceNumeric\") \\\n",
        ".sort(\"date\") \\\n",
        ".show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Lz4jGe94Xl"
      },
      "source": [
        "Finance's analysts saw these results, and told us that there is an error here. \n",
        "They did the calculations manually and told us that it is supposed to be like this:\n",
        "\n",
        "```\n",
        "\n",
        "+----------+-----------------+\n",
        "|date      |sum(priceNumeric)|\n",
        "+----------+-----------------+\n",
        "|2020-09-29|4880             |\n",
        "|2020-09-30|5000             |\n",
        "|2020-10-01|5120             |\n",
        "|2020-10-02|5190             |\n",
        "|2020-10-03|36610            |\n",
        "+----------+-----------------+\n",
        "\n",
        "```\n",
        "\n",
        "Please help us find the bug in the code above, and return the right results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZUap62QwEEt"
      },
      "source": [
        "### Instruction for Q3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eevy7a3iwK0v"
      },
      "source": [
        "Our developer had to join the results with dimentional table of categories. The join works, but its a bit slow, see if you can understand why and whether it can run faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEOOrFdSwCle",
        "outputId": "4747c59d-a9fe-441b-e555-cb4aac8c19ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|Category   |Value|\n",
            "+-----------+-----+\n",
            "|Small Money|10   |\n",
            "|Some Money |20   |\n",
            "|Nice Value |40   |\n",
            "|BigMoney   |70   |\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load the first table\n",
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")\n",
        "# create the next table\n",
        "ratesCategory = [('Small Money',10),('Some Money',20),('Nice Value',40),('BigMoney',70)]\n",
        "categoriesDF = spark.createDataFrame(ratesCategory,['Category','Value'])\n",
        "categoriesDF.show(10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt2Q1pj1w6F5",
        "outputId": "d9490051-3c16-4ffa-d262-7cdb3a69d3ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----+--------+-----+------------+----------+\n",
            "|Category   |Value|customer|price|priceNumeric|date      |\n",
            "+-----------+-----+--------+-----+------------+----------+\n",
            "|Small Money|10   |1981    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1978    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1967    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1949    |10   |10          |2020-10-01|\n",
            "|Small Money|10   |1937    |10   |10          |2020-10-01|\n",
            "+-----------+-----+--------+-----+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "bigDF = categoriesDF.join(broadcast(df), categoriesDF.Value == df.priceNumeric)\n",
        "bigDF.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instruction for Q3\n",
        "The following function first enriches the table with data and performs transformations.  \n",
        "Then it writes it 5 times, each time adding a column with a different name.   \n",
        "The function is running slower than expected, please help us optimize the function and make it run faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG1qCc_bx8wL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# -- recreate the table above - ignore this part\n",
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")\n",
        "ratesCategory = [('Small Money',10),('Some Money',20),('Nice Value',40),('BigMoney',70)]\n",
        "categoriesDF = spark.createDataFrame(ratesCategory,['Category','Value'])\n",
        "bigDF = categoriesDF.join(broadcast(df), categoriesDF.Value == df.priceNumeric)\n",
        "# -- \n",
        "\n",
        "from pyspark.sql.functions import sha2\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "# enrich our dataframe with additional data:\n",
        "enrichedDF = bigDF.withColumn(\"hash\", sha2(\"customer\", 256))\n",
        "window = Window.orderBy(\"hash\")\n",
        "enrichedDF = enrichedDF.withColumn(\"row_number\", row_number().over(window))\n",
        "\n",
        "# using this data, we are creating 5 different files with the same data and a different field \n",
        "fields = ['field1','field2','field3','field4','field5']\n",
        "\n",
        "for field in fields:\n",
        "  enrichedDF \\\n",
        "  .withColumn(\"field\", F.lit(field)) \\\n",
        "  .write \\\n",
        "  .option(\"header\",True) \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .csv(f\"transactions-postproc-{field}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Copy of Interview-Answers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
