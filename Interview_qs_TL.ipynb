{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interview-Answers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYVl8uVYAefbydm7Np3CXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-agmon/interviewdata/blob/main/Interview_qs_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Please start by running the following cells that will download the data and the Spark environment.\n",
        "#### Questions start after this part"
      ],
      "metadata": {
        "id": "zt00VArS0WAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkKbxoXeythI"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-01\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-02\n",
        "!wget -q https://raw.githubusercontent.com/a-agmon/interviewdata/main/daily-transactions-2020-10-03"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf transactions-postproc\n",
        "!rm -rf daily-transactions\n",
        "!mkdir daily-transactions\n",
        "!mv daily-*2020* daily-transactions/"
      ],
      "metadata": {
        "id": "srxA-UPRy4r8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tree\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A94t8YSBzNyy",
        "outputId": "72832ee3-6498-40ad-b8f4-7746d61e9524"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (151 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,310 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,937 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,369 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,095 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,533 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n",
            "Fetched 13.6 MB in 4s (3,700 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvB2Zqkzz-6L",
        "outputId": "80534d94-fa2b-4709-f751-6a2546d8e395"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 61.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=47416c29a1c580e597451bc11b460bd7c6152990bbdd0f07111e5ed27248a76a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "ivG5oW6o0XmB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting a spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "conf = SparkConf()"
      ],
      "metadata": {
        "id": "thKuQtUgAfbp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please also run these - they build the data structure for the first question"
      ],
      "metadata": {
        "id": "OMOGSnxpPewG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "packs =  [\n",
        "    {'pack':1, 'pack_start_date':123456, 'pack_end_date':123460, 'pack_installs':10},\n",
        "    {'pack':2, 'pack_start_date':123460, 'pack_end_date':123470, 'pack_installs':5},\n",
        "    {'pack':3, 'pack_start_date':123470, 'pack_end_date':123475, 'pack_installs':10}]\n",
        "\n",
        "consumption = [\n",
        "    {'account':'AB','install_date':123459, 'installs':10},\n",
        "    {'account':'AB','install_date':123465, 'installs':5},\n",
        "    {'account':'AB','install_date':123466, 'installs':3}]"
      ],
      "metadata": {
        "id": "0zW_MCGfPYAE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### * START HERE *"
      ],
      "metadata": {
        "id": "QPBj0VDsiU27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run these and view the generated tables\n",
        "\n",
        "packs_df = spark.createDataFrame(packs)\n",
        "consumption_df = spark.createDataFrame(consumption)\n",
        "\n",
        "print(\"Packages table\\n\")\n",
        "packs_df.show()\n",
        "print(\"consumption table\\n\")\n",
        "consumption_df.show()\n",
        "\n",
        "packs_df.createOrReplaceTempView(\"packs\")\n",
        "consumption_df.createOrReplaceTempView(\"consumption\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3fUaCg6Pa53",
        "outputId": "940fe755-3ced-4e98-8dc0-b966e260be1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages table\n",
            "\n",
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n",
            "consumption table\n",
            "\n",
            "+-------+------------+--------+\n",
            "|account|install_date|installs|\n",
            "+-------+------------+--------+\n",
            "|     AB|      123459|      10|\n",
            "|     AB|      123465|       5|\n",
            "|     AB|      123466|       3|\n",
            "+-------+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions for Q1\n",
        "\n",
        "The **Packages** table represents packages that customers purchase.\n",
        "Each package has an ID, a start and end date (represented by a number), and a number of installs that the package includes.\n",
        "\n",
        "The **Consumption** table shows us how many installs each account used and when. When we get consumption data for a user, then we need to check according to the date, which package the user used. A user can only have one package in any given time. \n",
        "\n",
        "The report we need to calculate needs to show how much installs a user used from each of its packages, and how many installs remain in each package the user purchased "
      ],
      "metadata": {
        "id": "jy8-_ThxpBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an example to how a spark query can run\n",
        "spark.sql(\"select * from packs\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tilfLIHGPp4L",
        "outputId": "edecf6ea-7dcc-4701-c5ab-876be3b9b48a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+-------------+---------------+\n",
            "|pack|pack_end_date|pack_installs|pack_start_date|\n",
            "+----+-------------+-------------+---------------+\n",
            "|   1|       123460|           10|         123456|\n",
            "|   2|       123470|            5|         123460|\n",
            "|   3|       123475|           10|         123470|\n",
            "+----+-------------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example for the report we want to see\n",
        "\n",
        "\n",
        "```\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|account|pack|InstallsInPackage|InstallsUsed|InstallsDelta|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "|     AB|   1|               10|          10|            0|\n",
        "|     AB|   2|                5|           8|           -3|\n",
        "+-------+----+-----------------+------------+-------------+\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "JzdbqT5NQAVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqlQuery = \"\"\"\n",
        "\n",
        "SELECT 1 + 1\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RxAxcYiSP8Ag"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(sqlQuery).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLHy04x1QcRb",
        "outputId": "1abd60c6-24a4-4736-a5bf-0c7ee0bf14b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|(1 + 1)|\n",
            "+-------+\n",
            "|      2|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instruction for Q2**\n",
        "\n",
        "A developer on the team wrote an ETL that runs once a day as a Spark job.\n",
        "Every day it reads a csv file that shows the total value of each customer's transactions of that day and write them as a parquet file partitioned by date and customer id.\n",
        "Below you can see an example of the CSV file. Note that each customer has one entry that represents the total sum of transaction value it did on that day.\n",
        "\n",
        "However, sometimes the csv file contains a correction for a sum reported in the past. \n",
        "\n",
        "for example - This file represents the transactions on the 1/10. You can see that **customer 1002** has 2 entries. One for the 1/10 and one for 30/9. This means that the total sum of transactions the customer did on the 1/10 is 70, but also that the total sum of transaction it did on the 30/9 was 40 and that this sum should **replace** the value already reported on the 30/9. \n",
        "\n",
        "\n",
        "```\n",
        "current date file: 2020-10-01\n",
        "\n",
        "date,customer,price\n",
        "2020-10-01,1000,40\n",
        "2020-10-01,1001,10\n",
        "2020-09-30,1002,40\n",
        "2020-10-01,1002,70\n",
        "2020-10-01,1003,10\n",
        "2020-09-29,1004,10\n",
        "2020-10-01,1004,10\n",
        "```\n",
        "\n",
        "After the transformations files written in this partitioning scheme based on date and customer id\n",
        "\n",
        "```\n",
        "|_date=2000-1-1\n",
        "|___customer=100\n",
        "|_______file.p\n",
        "|___customer=101\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZIXrlZ4d0_c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the folder that the prq files are written to\n",
        "# before running the ETL this should be cleared \n",
        "!rm -rf transactions-postproc/"
      ],
      "metadata": {
        "id": "x0O9zdz92Fb_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function represents the ETL. It runs once a day with a string represening the current day. \n",
        "\n",
        "It reads the csv file, does some transformations, and write it."
      ],
      "metadata": {
        "id": "H5W2GkNB86BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_etl(current_date): \n",
        "\n",
        "  df = spark.read.option(\"header\",True).csv(f\"daily-transactions/daily-transactions-{current_date}\")\n",
        "  \n",
        "  df = df.withColumn(\"priceNumeric\", F.col(\"price\").astype(IntegerType()))\n",
        "  \n",
        "  # some other transformation code \n",
        "\n",
        "  df.write \\\n",
        "  .option(\"header\",True) \\\n",
        "  .partitionBy(\"date\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .parquet(\"transactions-postproc\")"
      ],
      "metadata": {
        "id": "ZQ2RGvAp04Da"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell simulate the ETL running over 3 days for testing purposes"
      ],
      "metadata": {
        "id": "-qLM9lIH9QYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# takes a minute to run!\n",
        "days = ['2020-10-01', '2020-10-02', '2020-10-03']\n",
        "\n",
        "for date_str in days:\n",
        "  run_etl(date_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKu7BOc61aQK",
        "outputId": "2ff64a7a-7a9b-431c-c498-81a7b8fd853f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 69.8 ms, sys: 10.8 ms, total: 80.6 ms\n",
            "Wall time: 7.41 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the two lines below to test the results that should sum how much did the company made each day from all the customers"
      ],
      "metadata": {
        "id": "a2oKbMy69qsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\",True).parquet(\"transactions-postproc\")"
      ],
      "metadata": {
        "id": "wI9C2VRR3Jcd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"date\") \\\n",
        ".sum(\"priceNumeric\") \\\n",
        ".sort(\"date\") \\\n",
        ".show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNeTRt8j3RSl",
        "outputId": "9b6e81ed-2db1-4a53-ab14-cc4b777221b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+\n",
            "|date      |sum(priceNumeric)|\n",
            "+----------+-----------------+\n",
            "|2020-10-01|5120             |\n",
            "|2020-10-02|5190             |\n",
            "|2020-10-03|36610            |\n",
            "+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finance saw these results, and told us that there is an error here. They did the calculations manually and told us that it is supposed to be like this:\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "+----------+-----------------+\n",
        "|date      |sum(priceNumeric)|\n",
        "+----------+-----------------+\n",
        "|2020-09-29|4880             |\n",
        "|2020-09-30|9790             |\n",
        "|2020-10-01|35330            |\n",
        "|2020-10-02|32940            |\n",
        "|2020-10-03|36610            |\n",
        "+----------+-----------------+\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Please help us find the bug in the code above, and return the right results"
      ],
      "metadata": {
        "id": "f3Lz4jGe94Xl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions for Q3\n",
        "\n",
        "A developer on the team was running the follwing line in a function for logging purposes, and the job crashed with out of memory exception. \n",
        "The developer says that the cluster has many workers with a lot of memory and disk and still the job crashes.\n",
        "Can you help explain how come this line makes the job crash with OOM even though the cluster is huge?"
      ],
      "metadata": {
        "id": "647kSHW1B3AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def someFunc():\n",
        "  #.....\n",
        "  for row in df.collect():\n",
        "    print(f'Customerr{row[\"customer\"]} => Paid {row[\"price\"]}')"
      ],
      "metadata": {
        "id": "ViPXYkTOBLp_"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}